Cloud Computing for Data Analysis
----------------------------------

Group Activity 07 - Spark GraphX
---------------------------------

Creating Scala-Spark project and adding dependencies in Scala-IDE

	1.1. Open Scala-IDE
	1.2. Create New Maven project
	1.3. Right click on the project, click Configure and click "add Scala Nature" 
	1.4. Add the following dependencies in pom.xml.
		<dependencies>
  			<dependency>
  				<groupId>org.apache.spark</groupId>
  				<artifactId>spark-core_2.11</artifactId>
  				<version>2.1.2</version>
  			</dependency>
  			<dependency>
  				<groupId>org.apache.spark</groupId>
  				<artifactId>spark-graphx_2.11</artifactId>
  				<version>2.1.2</version>
  			</dependency>
  		</dependencies>
	1.5. Right click on the project and click "Properties"


AWS Setup Instructions:
1. Create a cluster with Hadoop and Spark in AWS and start the cluster. Once the cluster is running, log-in to the master node using Putty(Windows) or SSH(MAC or Linux)

************
NOTE: Please use the below configuration settings while creating cluster. This includes spark resource allocation settings(in addition to the existing yarn settings of previous exercises) to avoid heap space issue.

[{"classification":"yarn-site","properties":{"yarn.nodemanager.aux-services":"mapreduce_shuffle","yarn.nodemanager.aux-services.mapreduce_shuffle.class":"org.apache.hadoop.mapred.ShuffleHandler"}},{"classification":"spark","properties":{"maximizeResourceAllocation":"true"}}]
************

2. Create a data bucket in AWS S3. Upload all required input files and .jar files to the data bucket


Part-1:
1. Download GraphXBasics.scala from http://webpages.uncc.edu/aatzache/ITCS6190/Exercises/SparkGraphX/GraphXBasics.scala
	Copy this file into your project
	//This code executes a simple GraphX operations

2. Click RunAs and select Maven Clean  

3. Click RunAs and select Maven Install. This creates a .jar file inside 'target' folder in your project. Upload this .jar file to the data bucket

4. Login to the master node of the cluster in Terminal / Putty

5. From the master node download .jar using the command:
	aws s3 cp s3://BUCKET_NAME/JAR_FILE_NAME .

6. Run this .jar file using the following command
	spark-submit --master yarn --class "ClassName" JAR_FILE_NAME s3://BUCKET_NAME/OUTPUT_DIR_NAME
	
7. Give "PackageName" in the "ClassName" (Example: org.GraphX.Main) if you use any packages for the program	

8. Download the output from the data bucket in S3 and upload to Canvas


Part-2
1. Create a new Maven project and do necessary configurations as told before

2. Download GraphXShortestPath.scala from http://webpages.uncc.edu/aatzache/ITCS6190/Exercises/SparkGraphX/GraphXShortestPath.scala
	Copy this file into your project.
	//This code executes Dijkstra's Shortest path algorithm for a given dataset

2. Download 'animal_terms.txt' from http://webpages.uncc.edu/aatzache/ITCS6190/Exercises/SparkGraphX/animal_terms.txt
	//This file contain graph vertices(animal names)

3. Download 'animal_distances.txt' from http://webpages.uncc.edu/aatzache/ITCS6190/Exercises/SparkGraphX/animal_distances.txt
	//This file contain graph edges in the format of: (animalVertex1,animalVertex2,edge_weight) 

NOTE: We need to specify your hadoop home directory in the program, if you are running the code in Eclipse
NOTE: We should specify 3 argument values: 1: file path of animal_terms.txt , 2: file path of animal_distances.txt , 3: output path

4. Get a .jar file as mentioned in Part-1 and upload to the data bucket

5. Login to the master node of the cluster in Terminal / Putty

6. From the master node download .jar using the command:
	aws s3 cp s3://BUCKET_NAME/JAR_FILE_NAME .

7. To run the jar file in cluster, use:  
	spark-submit --master yarn --class "ClassName" JAR_FILE_NAME s3://BUCKET_NAME/INPUT_PATH_TO_VERTICES_FILE s3://BUCKET_NAME/INPUT_PATH_TO_EDGES_FILE OUTPUT_PATH

8. Download the Output from data bucket and upload to Canvas

9. Delete/Terminate the AWS cluster and delete all files from S3 when finished, otherwise Amazon will charge your Credit Card

Files to Submit:
1. Outputs of programs from Part-1 and Part-2
2. Programs
3. Commands, Errors(if any) from the terminal window
